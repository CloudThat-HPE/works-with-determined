{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/determined-ai/determined/master/determined-logo.png\" align='right' width=150 />\n",
    "\n",
    "# Building a Pedestrian Detection Model with Determined\n",
    "\n",
    "<img src=\"https://www.cis.upenn.edu/~jshi/ped_html/images/PennPed00071_1.png\" width=400 />\n",
    "\n",
    "\n",
    "This notebook will walk through the benefits of building a Deep Learning model with Determined.  We will build an object detection model trained on the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/) and deploy it to Algorithmia for serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from determined.experimental import Determined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the Determined cluster\n",
    "\n",
    "For our first example, we run a simple single-GPU training job with fixed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!det e create const.yaml ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determined Model Registry\n",
    "\n",
    "After training, we'll want to actually use our model in some sort of system.  Determined provides a model registry to version your trained models, making them easy to retrieve for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = < Experiment ID >\n",
    "MODEL_NAME = \"pedestrian_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "checkpoint = Determined().get_experiment(experiment_id).top_checkpoint()\n",
    "model = Determined().create_model(MODEL_NAME)\n",
    "model.register_version(checkpoint.uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Inference\n",
    "\n",
    "Once your model is versioned in the model registry, using that model for inference is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Determined().get_model(MODEL_NAME)\n",
    "trial = model.get_version().load()\n",
    "inference_model = trial.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import predict\n",
    "predict(inference_model, 'test.jpg', inference=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Endpoint\n",
    "\n",
    "Now that we can run inference in this notebook, let's set up a serving endpoint on Algorithmia so we can scale this model's serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll install Algorithmia and the associated utility packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Algorithmia\n",
    "!pip install retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algorithmia\n",
    "import algorithmia_utils\n",
    "import urllib.parse\n",
    "from git import Git, Repo, remote\n",
    "from retry import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll next save the model from the Determined Model Registry to this current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inference_model, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload your model and sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = <Your Algorithmia API Key>\n",
    "USERNAME = <Your Algorithmia username>\n",
    "\n",
    "client = Algorithmia.client(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLLECTION = \"pedestrian_detection\"\n",
    "DIRECTORY = client.dir(f\"data://{USERNAME}/{DATA_COLLECTION}\")\n",
    "if DIRECTORY.exists() is False:\n",
    "    DIRECTORY.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've created the data collection, you can upload your model and test image to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "MODEL_PATH = f\"data://{USERNAME}/{DATA_COLLECTION}/{MODEL_NAME}\"\n",
    "client.file(MODEL_PATH).putFile(MODEL_NAME)\n",
    "\n",
    "# Test image\n",
    "TEST_IMG_PATH = f\"data://{USERNAME}/{DATA_COLLECTION}/test.jpg\"\n",
    "client.file(TEST_IMG_PATH).putFile(\"test.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your serving Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithmia refers to each endpoint generically as an Algorithm. An Algorithm can be any executable code, in this case it is code that uses our model to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_NAME = \"pedestrian_algorithm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility = algorithmia_utils.AlgorithmiaUtils(API_KEY,\n",
    "                                                  USERNAME,\n",
    "                                                  ALGORITHM_NAME,\n",
    "                                                  local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility.create_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Algorithm is created, we can add the inference code that we want it to execute. Algorithmia supports editing the Algorithm code in your preferred IDE via its Github repo. You an either create a repo within your own Github account (if you link your Github account to Algorithmia), or you can use the repo created for you automatically by Algorithmia. For this example, we'll do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility.clone_algorithm_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithmia uses an script that will run when Algorithmia receives requests. We can write that out to the cloned repo using the `%%writefile` macro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $algo_utility.algo_script_path\n",
    "import Algorithmia\n",
    "import torch\n",
    "from .predict import predict\n",
    "\n",
    "# API calls will begin at the apply() method, with the request body passed as 'input'\n",
    "# For more details, see algorithmia.com/developers/algorithm-development/languages\n",
    "\n",
    "client = Algorithmia.client()\n",
    "\n",
    "def apply(input):\n",
    "    \n",
    "    img_path = input[\"img_path\"]\n",
    "    img = client.file(img_path).getFile().name\n",
    "    \n",
    "    model_path = input[\"model_path\"]\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    with open(img, 'rb') as img:\n",
    "        predictions = predict(model, img, inference=\"algorithmia\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def load_model(model_path):\n",
    "    \n",
    "    model_file = client.file(model_path).getFile().name\n",
    "    with open(model_file, 'rb') as model_file:\n",
    "        model = torch.load(model_file, map_location=torch.device('cpu'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can create a dependency file to ensure the Algorithm builds the correct runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $algo_utility.dependency_file_path\n",
    "algorithmia>=1.0.0,<2.0\n",
    "torch==1.5.1\n",
    "torchvision==0.6.1\n",
    "pillow\n",
    "numpy\n",
    "matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the actual predictions, we can continue to use the `predict.py` and `data.py` modules that we used earlier. We'll now just copy it directly into the repo so that it can be uploaded to the Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp predict.py ./{ALGORITHM_NAME}/src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to push the updated repo to Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_utility.push_algo_script_with_dependencies(filenames=[\n",
    "    f\"{ALGORITHM_NAME}.py\",\n",
    "    \"predict.py\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Algorithm created and inference code pushed, we can run inference on the test image we uploaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_result = algo_utility.call_latest_algo_version({\n",
    "    \"model_path\": MODEL_PATH,\n",
    "    \"img_path\": TEST_IMG_PATH\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo_result.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo_result.result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
